#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script pour nettoyer et extraire le dictionnaire Kabuverdianu depuis un PDF
avec un format de texte mal structur√© (tout sur une seule ligne).
"""

import PyPDF2
import re
import json
import os
import time
from typing import Tuple, List, Dict

try:
    import requests
except ImportError:
    requests = None
    import urllib.parse
    import urllib.request

def extract_text_from_pdf(pdf_path):
    """Extrait le texte brut d'un PDF"""
    text = ""
    try:
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            print(f"üìÑ Lecture du PDF: {len(pdf_reader.pages)} pages")
            
            for page_num, page in enumerate(pdf_reader.pages, 1):
                text += page.extract_text()
                if page_num % 10 == 0:
                    print(f"   Page {page_num}/{len(pdf_reader.pages)}...")
            
            print(f"‚úì Texte brut extrait: {len(text)} caract√®res")
    except Exception as e:
        print(f"‚ùå Erreur lors de la lecture du PDF: {e}")
        return None
    
    return text

def clean_text(raw_text):
    """Nettoie le texte en rempla√ßant les tabulations et retours chariot"""
    # Remplacer les multiples espaces/tabulations par un seul espace
    cleaned = re.sub(r'[\t\r]+', ' ', raw_text)
    # Ajouter des sauts de ligne avant chaque nouvelle entr√©e (mot + type)
    cleaned = re.sub(r'([a-z√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫√ß]+(?:\s+[a-z√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫√ß]+)?)\s+(n|v|adj|expr|adv|conj|prep|pro|excl|past\s+part)', r'\n\1 \2', cleaned)
    
    print(f"‚úì Texte nettoy√©: {len(cleaned)} caract√®res")
    return cleaned

def translate_to_portuguese(english_text):
    """
    Traduit les d√©finitions anglaises en portugais
    """
    translations = {
        'and': 'e',
        'or': 'ou',
        'the': 'o/a',
        'of': 'de',
        'to': 'para',
        'in': 'em',
        'on': 'em',
        'at': 'em',
        'with': 'com',
        'from': 'de',
        'by': 'por',
        'for': 'para',
        'about': 'sobre',
        'under': 'sob',
        'down below': 'abaixo',
        'brim': 'aba',
        'hat': 'chap√©u',
        'outskirts': 'arredores',
        'avocado': 'abacate',
        'shaking': 'agita√ß√£o',
        'shake': 'agitar',
        'touch': 'tocar',
        'abandon': 'abandonar',
        'advance': 'avan√ßo',
        'supply': 'fornecimento',
        'stock': 'estoque',
        'provisions': 'provis√µes',
        'bee': 'abelha',
        'honey': 'mel',
        'bless': 'aben√ßoar',
        'wish well': 'desejar bem',
        'open': 'aberto',
        'opening': 'abertura',
        'title': 't√≠tulo',
        'last name': 'sobrenome',
        'you': 'voc√™',
        'collision': 'colis√£o',
        'embrace': 'abra√ßo',
        'hug': 'abra√ßo',
        'kiss': 'beijo',
        'yawn': 'bocejar',
        'April': 'Abril',
        'absolve': 'absolver',
        'absolution': 'absolvi√ß√£o',
        'absurd': 'absurdo',
        'abound': 'abundar',
        'abuse': 'abusar',
        'insult': 'insulto',
        'adapt': 'adaptar',
        'goodbye': 'adeus',
        'admiration': 'admira√ß√£o',
        'noise': 'barulho',
        'sounds': 'sons',
        'screaming': 'gritaria',
        'adjective': 'adjetivo',
        'garlic': 'alho',
        'administrator': 'administrador',
        'administration': 'administra√ß√£o',
        'alphabet': 'alfabeto',
        'letter': 'letra',
        'letters': 'letras',
        'eternity': 'eternidade',
        'forehead': 'testa',
        'need': 'necessidade',
        'needs': 'necessidades',
        'needed': 'necess√°rio',
        'thing': 'coisa',
        'things': 'coisas',
        'must': 'deve',
        'should': 'deveria',
        'cause': 'causa',
        'because': 'porque',
        'after': 'depois',
        'before': 'antes',
        'during': 'durante',
        'always': 'sempre',
        'never': 'nunca',
        'sometimes': '√†s vezes',
        'soon': 'logo',
        'late': 'tarde',
        'early': 'cedo',
        'home': 'casa',
        'house': 'casa',
        'man': 'homem',
        'woman': 'mulher',
        'child': 'crian√ßa',
        'children': 'crian√ßas',
        'family': 'fam√≠lia',
        'people': 'pessoas',
        'person': 'pessoa',
        'friend': 'amigo',
        'friends': 'amigos',
        'brother': 'irm√£o',
        'sister': 'irm√£',
        'father': 'pai',
        'mother': 'm√£e',
        'day': 'dia',
        'night': 'noite',
        'week': 'semana',
        'month': 'm√™s',
        'year': 'ano',
        'years': 'anos'
    }
    
    result = english_text
    for eng, pt in translations.items():
        result = re.sub(r'\b' + eng + r'\b', pt, result, flags=re.IGNORECASE)

    return _normalize_pt_text(result)

def _normalize_pt_text(text: str) -> str:
    if not text:
        return ''
    normalized = text.replace('\xa0', ' ')
    normalized = re.sub(r'\s+', ' ', normalized)
    return normalized.strip()

def detect_english_tokens(text: str) -> List[str]:
    pattern = re.compile(r"\b[a-zA-Z]{2,}\b")
    allowed = {
        'de', 'do', 'da', 'das', 'dos', 'o', 'a', 'os', 'as', 'e', 'em', 'para', 'com', 'um', 'uma',
        'que', 'ser', 'ter', 'fazer', 'ir', 'estar', 'mais', 'menos', 'muito', 'pouco', 'grande',
        'pequeno', 'novo', 'velho', 'melhor', 'pior', 'nosso', 'nossa', 'nos', 'seu', 'sua', 'isso',
        'isto', 'aquele', 'aquela', 'estes', 'estas', 'esse', 'essa', 'eles', 'elas', 'eu', 'tu',
        'ele', 'ela', 'n√≥s', 'voc√™s', 'vos', 'se', 'j√°', 'n√£o', 'sim', 'onde', 'quando', 'como',
        'porque', 'pois', 'h√°', '√†s'
    }
    accent_pattern = re.compile(r"[√°√©√≠√≥√∫√£√µ√¢√™√¥√ß√†]", re.IGNORECASE)
    tokens: List[str] = []
    for token in pattern.findall(text or ''):
        lower = token.lower()
        if lower in allowed or accent_pattern.search(lower):
            continue
        tokens.append(token)
    return tokens

def remove_english_tokens(text: str, tokens: List[str]) -> str:
    cleaned = text or ''
    cleaned = cleaned.replace('\xa0', ' ')
    for token in set(tokens):
        cleaned = re.sub(r'\b' + re.escape(token) + r'\b', '', cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r'\s{2,}', ' ', cleaned)
    return cleaned.strip()

def load_translation_cache(path: str) -> Dict[str, str]:
    if not os.path.exists(path):
        return {}
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return {}

def save_translation_cache(path: str, cache: Dict[str, str]):
    try:
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as exc:
        print(f"‚ö†Ô∏è Impossible d'enregistrer le cache de traduction: {exc}")

def machine_translate_to_pt(text: str, cache: Dict[str, str]) -> str:
    if not text:
        return ''
    key = text.strip()
    if not key:
        return ''
    if key in cache:
        return cache[key]
    try:
        if requests:
            resp = requests.get(
                'https://api.mymemory.translated.net/get',
                params={'q': key, 'langpair': 'en|pt'},
                timeout=10
            )
            data = resp.json()
        else:
            qs = urllib.parse.urlencode({'q': key, 'langpair': 'en|pt'})
            url = f'https://api.mymemory.translated.net/get?{qs}'
            with urllib.request.urlopen(url, timeout=10) as resp:  # nosec B310
                data = json.loads(resp.read().decode('utf-8'))
        translated = data.get('responseData', {}).get('translatedText') or ''
        cache[key] = translated
        time.sleep(0.3)
        return translated
    except Exception as exc:
        print(f"‚ö†Ô∏è Erreur de traduction automatique: {exc}")
        return ''

def _normalize_word(w: str) -> str:
    return re.sub(r"[^a-z0-9]+", " ", w.lower()).strip()

def generate_example_with_ai(word: str, pt_translation: str, wtype: str) -> Tuple[str, str]:
    """G√©n√®re un exemple PT/CV de fa√ßon d√©terministe (sans API externe).

    - word: mot en CV
    - pt_translation: d√©finition/√©quivalent en PT
    - wtype: type lexical d√©tect√© (n, v, adj, ...)
    """
    # Choisir un gabarit basique selon le type
    base_pt = pt_translation.split(';')[0].split('.')[0].strip()
    norm = _normalize_word(base_pt)
    # Quelques verbes auxiliaires en PT/CV pour rendre la phrase naturelle
    if re.fullmatch(r"v|verb|verbo", wtype, flags=re.IGNORECASE):
        # Verbes: progressivo/a√ß√£o presente
        # PT: "Eu estou a {verbo}"; CV: "N sta {word}"
        # Se a tradu√ß√£o come√ßa com "ser ", usar forma de estado
        if norm.startswith("ser "):
            pt = f"Eu sou {norm[4:]}"
            cv = f"N √© {word}"
        else:
            pt = f"Eu estou a {norm}"
            cv = f"N sta {word}"
    elif re.fullmatch(r"adj|adjetivo", wtype, flags=re.IGNORECASE):
        pt = f"Isto √© {norm}" if norm else f"Isto √© bom"
        cv = f"Es √© {word}"
    else:  # nom, expr, adv, etc.
        # Noms/expressions: en PT utiliser "Tenho um/uma" ou "Esta/Este"
        if norm:
            pt = f"Tenho um/uma {norm}"
        else:
            pt = "Isto √© importante"
        cv = f"N ten un {word}"
    return pt.strip().capitalize() + ".", cv.strip().capitalize() + "."

def parse_dictionary(text):
    """
    Parse le texte nettoy√© du dictionnaire
    """
    entries = []
    lines = text.split('\n')
    entry_id = 1
    
    word_types = r'(n|v|adj|expr|adv|conj|prep|pro|excl|past\s+part)'
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        entry_match = re.match(r'^([a-z√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫√ß]+(?:\s+[a-z√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫√ß]+)?)\s+' + word_types + r'\s+(.+)$', line, re.IGNORECASE)
        
        if entry_match:
            word = entry_match.group(1).strip()
            wtype = entry_match.group(2).strip()
            definition = entry_match.group(3).strip()
            
            pt_definition = translate_to_portuguese(definition).strip()

            override_pt = OVERRIDDEN_DICTIONARY.get(word.lower())
            if override_pt:
                pt_definition = override_pt
            else:
                english_tokens = detect_english_tokens(pt_definition)
                if english_tokens:
                    auto_pt = machine_translate_to_pt(definition, parse_dictionary.translation_cache)
                    if auto_pt:
                        pt_definition = auto_pt.strip()
                        english_tokens = detect_english_tokens(pt_definition)
                    if english_tokens:
                        pt_definition = remove_english_tokens(pt_definition, english_tokens)

            if not pt_definition:
                pt_definition = 'Defini√ß√£o indispon√≠vel'

            if detect_english_tokens(pt_definition):
                pt_definition = 'Defini√ß√£o indispon√≠vel'
            
            # G√©n√©rer les exemples PT/CV via IA simul√©e
            ex_pt, ex_cv = generate_example_with_ai(word, pt_definition, wtype)
            
            entries.append({
                'id': f'entry-{entry_id}',
                'word': word,
                'translation': {
                    'pt': pt_definition,
                    'cv': word
                },
                'example': {
                    'pt': ex_pt,
                    'cv': ex_cv
                }
            })
            entry_id += 1
            
            if entry_id % 100 == 0:
                print(f"   {entry_id} mots extraits et exemples g√©n√©r√©s...")
    
    return entries

parse_dictionary.translation_cache: Dict[str, str] = {}

def load_overrides(path: str) -> dict:
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return {}

def apply_overrides_and_deduplicate(entries: list, overrides: dict) -> Tuple[list, dict]:
    """Applique overrides.json, supprime doublons par 'word' (insensible √† la casse).
    Retourne (entries_nettoy√©es, stats)."""
    seen = {}
    duplicates = 0
    overridden = 0
    for e in entries:
        key = e['word'].lower()
        if key in seen:
            duplicates += 1
            # Ignorer l'entr√©e dupliqu√©e
            continue
        # Overrides
        ov = overrides.get(e['word']) or overrides.get(key)
        if ov:
            overridden += 1
            if 'translation_pt' in ov and ov['translation_pt']:
                e['translation']['pt'] = ov['translation_pt']
            if 'example' in ov and isinstance(ov['example'], dict):
                e['example']['pt'] = ov['example'].get('pt', e['example']['pt'])
                e['example']['cv'] = ov['example'].get('cv', e['example']['cv'])
            if 'category' in ov and ov['category']:
                e['category'] = ov['category']
        seen[key] = e
    cleaned = list(seen.values())
    stats = { 'duplicates_removed': duplicates, 'overrides_applied': overridden, 'final_count': len(cleaned) }
    return cleaned, stats

def validate_entries(entries: list) -> dict:
    """V√©rifie des probl√®mes courants et renvoie des stats/√©chantillons."""
    empty_examples = []
    bad_translations = []
    english_tokens = []

    for e in entries:
        if not e['example']['pt'] or not e['example']['cv']:
            empty_examples.append(e['word'])

        pt_text = e['translation']['pt']
        if re.search(r"\b(the|and|of|to|with|from)\b", pt_text, re.IGNORECASE):
            bad_translations.append(e['word'])

        tokens_found = detect_english_tokens(pt_text)
        if tokens_found:
            english_tokens.append({
                'word': e['word'],
                'tokens': tokens_found,
                'translation': pt_text
            })

    return {
        'empty_examples_count': len(empty_examples),
        'bad_translations_count': len(bad_translations),
        'empty_examples_sample': empty_examples[:20],
        'bad_translations_sample': bad_translations[:20],
        'english_tokens_count': len(english_tokens),
        'english_tokens_sample': english_tokens[:20],
    }

def generate_typescript_file(entries, output_path):
    """G√©n√®re le fichier TypeScript (tri alphab√©tique par 'word')."""
    entries_sorted = sorted(entries, key=lambda e: e['word'].lower())
    json_data = json.dumps(entries_sorted, ensure_ascii=False, indent=2)
    
    ts_content = f"""// Fichier g√©n√©r√© automatiquement depuis le PDF
// Ne pas modifier manuellement

export interface DictionaryEntry {{
  id: string;
  word: string;
  translation: {{
    pt: string;
    cv: string;
  }};
  example: {{
    pt: string;
    cv: string;
  }};
  note?: string;
  category?: string;
}}

export const dictionaryData: DictionaryEntry[] = {json_data};

export default dictionaryData;
"""
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(ts_content)
    
    print(f"‚úÖ Fichier g√©n√©r√©: {output_path}")
    print(f"   {len(entries_sorted)} entr√©es cr√©√©es (tri√©es)")

def main():
    print("üöÄ Nettoyage et extraction du dictionnaire...\n")
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_dir = os.path.dirname(script_dir)
    pdf_path = os.path.join(project_dir, 'pdfs', 'dicionario.pdf')
    output_path = os.path.join(project_dir, 'src', 'data', 'dictionaryData.ts')
    
    if not os.path.exists(pdf_path):
        print(f"‚ùå PDF non trouv√©: {pdf_path}")
        return
    
    raw_text = extract_text_from_pdf(pdf_path)
    if not raw_text:
        return
    
    cleaned_text = clean_text(raw_text)
    
    print("\nüìù Parsing du dictionnaire...")
    # Pr√©parer le cache de traduction automatique
    translation_cache_path = os.path.join(script_dir, 'translation_cache.json')
    parse_dictionary.translation_cache = load_translation_cache(translation_cache_path)

    entries = parse_dictionary(cleaned_text)
    
    # Overrides + d√©duplication (fusion overrides.json + overrides-jw.json)
    overrides_path = os.path.join(script_dir, 'overrides.json')
    overrides_jw_path = os.path.join(script_dir, 'overrides-jw.json')
    overrides = load_overrides(overrides_path)
    overrides_jw = load_overrides(overrides_jw_path)
    try:
        # fusion simple (JW a priorit√©)
        overrides.update(overrides_jw)
    except Exception:
        pass
    entries, ov_stats = apply_overrides_and_deduplicate(entries, overrides)
    
    if not entries:
        print("‚ùå Aucune entr√©e extraite")
        return
    
    print(f"‚úì {len(entries)} entr√©es apr√®s d√©duplication (overrides appliqu√©s: {ov_stats['overrides_applied']}, doublons supprim√©s: {ov_stats['duplicates_removed']})")
    
    # Validation basique
    report = validate_entries(entries)
    report_path = os.path.join(script_dir, 'dictionary_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("Validation du dictionnaire\n")
        f.write(f"Total: {len(entries)}\n")
        f.write(f"Exemples vides: {report['empty_examples_count']}\n")
        f.write(f"Traductions suspectes: {report['bad_translations_count']}\n\n")
        f.write("Exemples vides (√©chantillon):\n")
        for w in report['empty_examples_sample']:
            f.write(f"- {w}\n")
        f.write("\nTraductions suspectes (√©chantillon):\n")
        for w in report['bad_translations_sample']:
            f.write(f"- {w}\n")
        f.write("\nTraductions contenant des tokens anglais (total: ")
        f.write(str(report['english_tokens_count']))
        f.write(")\n")
        for item in report['english_tokens_sample']:
            f.write(f"- {item['word']} -> {', '.join(item['tokens'])} :: {item['translation']}\n")
    print(f"üß™ Rapport de validation: {report_path}")
    
    print("\nüìÑ G√©n√©ration du fichier TypeScript...")
    generate_typescript_file(entries, output_path)
    
    print("\n‚úÖ Extraction termin√©e avec succ√®s!")

    save_translation_cache(translation_cache_path, parse_dictionary.translation_cache)

if __name__ == '__main__':
    main()
